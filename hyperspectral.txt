Reduction in Dimensions of Hyperspectral Images



Authors Names: Rishabh Chhabra  (13116055)
               Shashank Jaiswal (13116059)
               Shubham Raj      (13116064)




Abstract:  As the use of hyperspectral imaging techniques increases for various applications like astronomy, agriculture, biomedical imaging and mineralogy it becomes extremely important to reduce the complexity of the data obtained. Complex data increases computation time by several manifolds and hence it is difficult to process this data. One way to reduce the complexity of this data is to reduce the dimensionality of the hyperspectral images. This paper focuses on two linear algorithms for the same, name Principal Component Analysis (PCA) and Information Gain (IG).

Introduction:  Satellites transacts huge volumes of data between different sources. So reduction of these stack is necessary. Hyperspectral data use a number of dimensions which can be reduced by mapping them into lower order of dimensions without actually losing any considerable amount. Principal Component Analysis (PCA) is perhaps the most popular dimension reduction technique for hyperspectral data. The growth in data volumes due to the large increase in the spectral bands and high computational demands of PCA has prompted the need to develop a fast and efficient algorithm for PCA. In this work, we present on an implementation of Information Gain with PCA dimension reduction of hyper spectral data


PCA: It is the optimal linear scheme for reducing a set of high dimensional vectors into a set of lower dimensional vectors.The objective of PCA is to perform dimensionality reduction while preserving as much of the randomness in the high-dimensional space as possible. These lower dimensional vectors form a new coordinate system, called the 'principal components'. Given a set of data on n dimensions, PCA aims to find a linear subspace of dimension d lower than n such that the data points lie mainly on this linear subspace There are two types of methods for performing PCA, the matrix method, and the data method. In this work, we will focus on the matrix methods.

1. Compute covariance matrix Cx , CX ≡ (1/n)(X*X^T)
2. We select the matrix P to be a matrix where each row pi
is an eigenvector of CX
3. If A is a square matrix, a non-zero vector v is an eigenvector of A if there is a scalar λ such that Av = λv
4. Reduction: there are m eigenvectors, we reduce from m dimensions to k dimensions by choosing k eigenvectors related with k largest eigenvalues λ

k is calculated using proportion of variance (λ1+λ2+···λk)/(λ1+λ2+···λk+···+λm). Stop at PoV > 0.9. Scree graph plots of PoV vs k, stop at “elbow”.

Information gain : The information gain for an attribute is defined in terms of entropy as follows: The mutual information is equal to the total entropy for an attribute if for each of the attribute values a unique classification can be made for the result attribute. It is usually a good measure for deciding the relevance of an attribute (in our case a band). 

Mathematically ---

IG(T,a) = H(T) - H(T/a)         where IG- Information Gain, H(T)- total entropy and H(T/a)- entropy due to attribute 'a'. 

Information Gain(main description) :
Information Gain is a measure of dependence between the feature and the class label. It is one of the most popular feature selection techniques as it is easy to compute and simple to interpret. The information gain of a given attribute X with respect to the class attribute Y is the reduction in uncertainty about the value of Y when we know the value of X, IG(X ,Y). So information gain of a feature or band X and the class labels Y is calculated as

IG(X,Y) = H(X) - H(X/Y)               (1)


Entropy (H) is a measure of the uncertainty associated with a random variable. H(X)
and H(X|Y) is the entropy of band X and the entropy of band X after observing Class
Y , respectively calculated as

H(X) = -$p(x1)log(p(x1))               (2)  // where $- summation p(x)- probability and log is actually to the base '2'.
H(X/Y) = -$p(y1)$p(x1/y1)log(p(x1/y1)) (3)
       
The maximum value of information gain is 1. A feature with a high information gain is relevant. Information gain is evaluated independently for each feature and the features with the top-k values are selected as the relevant features. Information Gain does not eliminate redundant features.

x

Combining IG and PCA
Dimensionality reduction is divided into two main domains, feature selection and feature extraction. Feature selection tries to find a subset of the original higher order variables and feature extraction transforms the data in high dimensional space to a lower dimensional. A method combining the approaches taken by both the techniques is proposed such that
X Band Selected = PCA of Band ∩ IG of Band 

This method finds a subset of lower dimensions and transforms it into further lower dimensions. 

The formulation are as follows:-

For PCA----

The vector representation of image pixel vector is given by--

xi = [x1,x2,.....,xn]<sup>T</sup><sub>i</sub>

with all the pixel values be taken from correspondingg hyperspectral dimension. For huperspectral image with 'm' rows and 'n' columns of pixel then total will be M=m*n . The mean vector of all the image vector is

m = (1/M)$ xi

Cov(x) = E{(x-E(x))(x-E(x))<sup>T</sup>
 therefore Cx = (1/M)$(xk-m)(xk-m)<sup>T</sup>   




where pixel vector zi will form the first K bands of the PCA images.
Such formed PCA bands have the highest contrast or variance in the first band and the lowest contrast or variance in the last band. Therefore, the first K PCA bands often contain the majority of information residing in the original hyperspectral images and can be used for more effective and accurate analyses because the number of image bands and the amount of image noise involved are reduced. 

After this the new set of k bands are classified through Information gain removing the band with less information and more noise by setting threshold gain. Hence the number of bands are reduced.

X Band Selected = PCA of Band ∩ IG of Band



Conclusion

The study reveals that the combination of PCA and IG is a useful process for dimensionality reduction when compared to PCA or IG applied alone. Among
all the obtained PCA bands, the first few (about 5) bands may contain most of the information contained in the entire hyperspectral image data. After the first 10 PCA bands, all other bands contain mostly noise. Classifications using these first 5 PCA bands yield accurate results and patterns of hyperspectral images. Misclassifications using PCA occur mainly due to non linear nature of data. The use of IG to further classify data ignificantly reduces the amount of data to be handled and achieves practically acceptable and accurate classification results that are comparable with those obtained using the entire hyperspectral image data. The complexity of the algorithm used to implement PCA takes up most of the time but the compromise doesn't overweigh the reduction in data to be handled and thus PCA is considered as a beneficial option. Comparing the classification results done by PCA-IG is done using clustering. The clusters formed using the unreduced set of hyperspectral data and the data whose dimensions were reduced using PCA-IG techniques were found to be close. The proximity of the clusters are not as expected when the data has non linear characteristics or when any of the assumptions including those of principal components being orthogonal are not satisfied. Future research efforts will include adding more quantitative and qualitative measures to the PCA-IG technique to further refine the process of dimensionality reduction.    		
