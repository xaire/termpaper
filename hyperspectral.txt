Reduction in Dimensions of Hyperspectral Images



Authors Names: Rishabh Chhabra  (13116055)
               Shashank Jaiswal (13116059)
               Shubham Raj      (13116064)




Abstract:  As the use of hyperspectral imaging techniques increases for various applications like astronomy, agriculture, biomedical imaging and mineralogy it becomes extremely important to reduce the complexity of the data obtained. Complex data increases computation time by several manifolds and hence it is difficult to process this data. One way to reduce the complexity of this data is to reduce the dimensionality of the hyperspectral images. This paper focuses on two linear algorithms for the same, name Principal Component Analysis (PCA) and Information Gain (IG).

Introduction:  Satellites transacts huge volumes of data between different sources. So reduction of these stack is necessary. Hyperspectral data use a number of dimensions which can be reduced by mapping them into lower order of dimensions without actually losing any considerable amount. Principal Component Analysis (PCA) is perhaps the most popular dimension reduction technique for hyperspectral data. The growth in data volumes due to the large increase in the spectral bands and high computational demands of PCA has prompted the need to develop a fast and efficient algorithm for PCA. In this work, we present on an implementation of Information Gain with PCA dimension reduction of hyper spectral data


PCA: It is the optimal linear scheme for reducing a set of high dimensional vectors into a set of lower dimensional vectors.The objective of PCA is to perform dimensionality reduction while preserving as much of the randomness in the high-dimensional space as possible. These lower dimensional vectors form a new coordinate system, called the 'principal components'. Given a set of data on n dimensions, PCA aims to find a linear subspace of dimension d lower than n such that the data points lie mainly on this linear subspace There are two types of methods for performing PCA, the matrix method, and the data method. In this work, we will focus on the matrix methods.

1. Compute covariance matrix Cx , CX ≡ (1/n)(X*X^T)
2. We select the matrix P to be a matrix where each row pi
is an eigenvector of CX
3. If A is a square matrix, a non-zero vector v is an eigenvector of A if there is a scalar λ such that Av = λv
4. Reduction: there are m eigenvectors, we reduce from m dimensions to k dimensions by choosing k eigenvectors related with k largest eigenvalues λ

k is calculated using proportion of variance (λ1+λ2+···λk)/(λ1+λ2+···λk+···+λm). Stop at PoV > 0.9. Scree graph plots of PoV vs k, stop at “elbow”.

Information gain : The information gain for an attribute is defined in terms of entropy as follows: The mutual information is equal to the total entropy for an attribute if for each of the attribute values a unique classification can be made for the result attribute. It is usually a good measure for deciding the relevance of an attribute (in our case a band). 

Mathematically ---

IG(T,a) = H(T) - H(T/a)         where IG- Information Gain, H(T)- total entropy and H(T/a)- entropy due to attribute 'a'. 




Information Gain(main discription) :
Information Gain is a measure of dependence between the feature and the class label. It is one of the most popular feature selection techniques as it is easy to compute and simple to interpret. The information gain of a given attribute X with respect to the class attribute Y is the reduction in uncertainty about the value of Y when we know the value of X, IG(X ,Y). So information gain of a feature or band X and the class labels Y is calculated as

IG(X,Y) = H(X) - H(X/Y)               (1)


Entropy (H) is a measure of the uncertainty associated with a random variable. H(X)
and H(X|Y) is the entropy of band X and the entropy of band X after observing Class
Y , respectively calculated as

H(X) = -$p(x1)log(p(x1))               (2)  // where $- summation p(x)- probability and log is actually to the base '2'.
H(X/Y) = -$p(y1)$p(x1/y1)log(p(x1/y1)) (3)
       
The maximum value of information gain is 1. A feature with a high information gain is relevant. Information gain is evaluated independently for each feature and the features with the top-k values are selected as the relevant features. Information Gain does not eliminate redundant features.



